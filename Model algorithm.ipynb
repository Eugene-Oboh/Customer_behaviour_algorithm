{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7142288a",
   "metadata": {},
   "source": [
    "## Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "1b266ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6d0eb",
   "metadata": {},
   "source": [
    "## Create a Class for Batching\n",
    "This class determines how the data is splitted into batches and fed to the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "eb065ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Audiobooks_Data_Reader():\n",
    "    #This function loads the initial batch from the .npz files \n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "    \n",
    "        npz = np.load('audiobook_data_{0}.npz' .format(dataset))\n",
    "        #define inputs and targets and data types\n",
    "        self.inputs = npz['inputs'].astype(np.float)\n",
    "        self.targets = npz['targets'].astype(np.int)\n",
    "\n",
    "        # write a while loop which counts the batch number, determined by the batch size\n",
    "        # when validating or testing we take the whole data in a single batch in that cass batch_size = None\n",
    "        if batch_size == None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.current_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    # A function which loads the next batches\n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.batch_count:\n",
    "            self.current_batch = 0\n",
    "            raise StopIteration()\n",
    "        \n",
    "        batch_slice = slice(self.current_batch * self.batch_size, (self.current_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.current_batch += 1\n",
    "        \n",
    "        # One-hot encoding the targets\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbc25a",
   "metadata": {},
   "source": [
    "## Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "4ed65ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eugen\\AppData\\Local\\Temp\\ipykernel_11144\\3946767040.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.inputs = npz['inputs'].astype(np.float)\n",
      "C:\\Users\\eugen\\AppData\\Local\\Temp\\ipykernel_11144\\3946767040.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.targets = npz['targets'].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "#declare the shape outline of model\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 30\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# declare the placeholders\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "#create the first hidden layer\n",
    "# Activation function is Leaky_RELU\n",
    "weight_1 = tf.get_variable('weight_1', [input_size,hidden_layer_size])\n",
    "biases_1 = tf.get_variable('biases_1', [hidden_layer_size])\n",
    "output_1 = tf.nn.leaky_relu(tf.matmul(inputs, weight_1) + biases_1)\n",
    "\n",
    "#create second hidden layer\n",
    "# Activation function is Leaky_RELU\n",
    "weight_2 = tf.get_variable('weight_2', [hidden_layer_size,  hidden_layer_size])\n",
    "biases_2 = tf.get_variable('biases_2', [hidden_layer_size])\n",
    "output_2 = tf.nn.leaky_relu(tf.matmul(output_1, weight_2) + biases_2)\n",
    "\n",
    "#create third hidden layer\n",
    "# Activation function is Leaky_RELU\n",
    "weight_3 = tf.get_variable('weight_3', [hidden_layer_size,  hidden_layer_size])\n",
    "biases_3 = tf.get_variable('biases_3', [hidden_layer_size])\n",
    "output_3 = tf.nn.leaky_relu(tf.matmul(output_2, weight_3) + biases_3)\n",
    "\n",
    "#create fourth hidden layer\n",
    "# Activation function is Leaky_RELU\n",
    "weight_4 = tf.get_variable('weight_4', [hidden_layer_size,  hidden_layer_size])\n",
    "biases_4 = tf.get_variable('biases_4', [hidden_layer_size])\n",
    "output_4 = tf.nn.leaky_relu(tf.matmul(output_3, weight_4) + biases_4)\n",
    "\n",
    "#create fifth hidden layer\n",
    "# Activation function is Leaky_RELU\n",
    "weight_5 = tf.get_variable('weight_5', [hidden_layer_size,  hidden_layer_size])\n",
    "biases_5 = tf.get_variable('biases_5', [hidden_layer_size])\n",
    "output_5 = tf.nn.leaky_relu(tf.matmul(output_4, weight_5) + biases_5)\n",
    "\n",
    "# lets create output layer\n",
    "# Activation function is Leaky_RELU\n",
    "weight_6 = tf.get_variable('weight_6', [hidden_layer_size,  output_size])\n",
    "biases_6 = tf.get_variable('biases_6', [output_size])\n",
    "outputs = tf.matmul(output_5, weight_6) + biases_6\n",
    "\n",
    "#apply activatiotion function and calculate objective function of the final layer\n",
    "loss =tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets)\n",
    "# reduce loss to the mean. this boasts performance (tf.reduce_mean() is a func to  find the mean of atensor)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "#Choose the Optimization function and implement Batching\n",
    "optimize = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(mean_loss)\n",
    "\n",
    "#calculate the accuracy\n",
    "#fist check if the outputs equals the targets\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, axis =1), tf.argmax(targets, axis=1))\n",
    "\n",
    "\n",
    "# the accuracy is the mean of the out_equals_target tensor\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# set the batch size and early stopping\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 50\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "# Loading the data using the Audiobooks_Data_Reader class\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size )\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0591c",
   "metadata": {},
   "source": [
    "## Creating loop to Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0cb1b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.616. Validation loss: 0.533. Validation accuracy: 70.69%\n",
      "Epoch 2. Training loss: 0.459. Validation loss: 0.401. Validation accuracy: 80.54%\n",
      "Epoch 3. Training loss: 0.376. Validation loss: 0.367. Validation accuracy: 81.66%\n",
      "Epoch 4. Training loss: 0.358. Validation loss: 0.353. Validation accuracy: 81.66%\n",
      "Epoch 5. Training loss: 0.353. Validation loss: 0.339. Validation accuracy: 81.88%\n",
      "Epoch 6. Training loss: 0.347. Validation loss: 0.332. Validation accuracy: 82.55%\n",
      "Epoch 7. Training loss: 0.341. Validation loss: 0.327. Validation accuracy: 82.33%\n",
      "Epoch 8. Training loss: 0.337. Validation loss: 0.324. Validation accuracy: 82.10%\n",
      "Epoch 9. Training loss: 0.335. Validation loss: 0.321. Validation accuracy: 82.55%\n",
      "Epoch 10. Training loss: 0.333. Validation loss: 0.319. Validation accuracy: 82.10%\n",
      "Epoch 11. Training loss: 0.331. Validation loss: 0.317. Validation accuracy: 82.33%\n",
      "Epoch 12. Training loss: 0.329. Validation loss: 0.316. Validation accuracy: 82.77%\n",
      "Epoch 13. Training loss: 0.328. Validation loss: 0.315. Validation accuracy: 82.77%\n",
      "Epoch 14. Training loss: 0.327. Validation loss: 0.314. Validation accuracy: 83.45%\n",
      "Epoch 15. Training loss: 0.325. Validation loss: 0.313. Validation accuracy: 83.00%\n",
      "Epoch 16. Training loss: 0.325. Validation loss: 0.312. Validation accuracy: 83.45%\n",
      "Epoch 17. Training loss: 0.323. Validation loss: 0.312. Validation accuracy: 83.45%\n",
      "Epoch 18. Training loss: 0.322. Validation loss: 0.311. Validation accuracy: 83.45%\n",
      "Epoch 19. Training loss: 0.322. Validation loss: 0.311. Validation accuracy: 83.67%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "        \n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "    \n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})   \n",
    "        \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    prev_validation_loss = validation_loss\n",
    "    \n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a5a4e",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "971f2fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eugen\\AppData\\Local\\Temp\\ipykernel_11144\\3946767040.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.inputs = npz['inputs'].astype(np.float)\n",
      "C:\\Users\\eugen\\AppData\\Local\\Temp\\ipykernel_11144\\3946767040.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.targets = npz['targets'].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "#to test the model we forward propagate with the test data\n",
    "test_data = Audiobooks_Data_Reader('test')\n",
    "for input_batch, target_batch in test_data:\n",
    "       test_accuracy = sess.run([accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch}) \n",
    "        \n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "# Print the test accuracy\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f13683",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, the machine learning algorithm developed to predict whether an Audio book customer will buy again has shown promising results, achieving a text accuracy of 82.81%. While there is room for improvement, this algorithm has the potential to help businesses better understand their customers' behavior and preferences, allowing them to tailor their marketing strategies accordingly. With further refinement and application, this algorithm has the potential to be a valuable tool for businesses seeking to enhance customer retention and drive revenue growth. \n",
    "\n",
    "Note: Code resue for official purpose requires prior permission from Eugene  Oboh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26e4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
